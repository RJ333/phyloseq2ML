#' Run ranger with parameters of data.frame rows.
#'
#' This functions calls ranger using the parameter values in each row of the 
#' provided master_grid, using the data of the list elements.
#'
#' @param Target The respective column from the master_grid
#' @param ML_object The respective column from the master_grid
#' @param Cycle The respective column from the master_grid
#' @param Number_of_trees The respective column from the master_grid
#' @param Mtry_factor The respective column from the master_grid
#' @param .row current row of master_grid
#' @param the_list The input tables list
#' @param master_grid the data frame containing all parameter combinations
#' @param step character declaring `training` or `prediction`
#' @param ... further parameters passed on to subfunctions
#'
#' @return a data frame with results and metrics for each row of the master_grid
#'
#' @export
ranger_classification <- function(master_grid, Target, ML_object, Cycle, Number_of_trees, 
  Mtry_factor, .row, the_list, step, ...) {

  stopifnot(step == "training" | step == "prediction")
  
  print(paste(.row, "of", nrow(master_grid)))
  all_vars <- ncol(the_list[[ML_object]][["train_set"]]) - 1
  for_mtry <- ifelse((sqrt(all_vars) * Mtry_factor) < all_vars,
    sqrt(all_vars) * Mtry_factor, all_vars)
  n_classes <- length(levels(as.factor(the_list[[ML_object]][["train_set"]][[Target]])))
  print(paste("classes:", n_classes))

  timing_part_1 <- system.time({
    RF_train <- ranger::ranger(
      dependent.variable.name = Target, 
      data = the_list[[ML_object]][["train_set"]],  # referring to the named object in the list
      num.trees = Number_of_trees,
      mtry = for_mtry,  
      importance = "none")
  })
  #print(RF_train)
  if (step == "prediction") {
    timing_part_2 <- system.time({
      RF_prediction <- stats::predict(object = RF_train, 
        data = the_list[[ML_object]][["test_set"]])
    })
    print(RF_prediction)
    confusion_matrix <- table(true = the_list[[ML_object]][["test_set"]][[Target]], 
      predicted = RF_prediction$predictions)
    store_multi(trained_rf = RF_train, predicted_rf = RF_prediction, 
      confusion_matrix = confusion_matrix, test_set = the_list[[ML_object]][["test_set"]],
      timings = timing_part_1 + timing_part_2, n_classes = n_classes, step = step)
  } else {  
    store_multi(trained_rf = RF_train, confusion_matrix = RF_train$confusion.matrix,
      timings = timing_part_1, n_classes = n_classes, step = step)
  }
}

#' Store results from ranger classification training and prediction
#'
#' This function extracts information from the ranger objects generated by
#' or prediction and stores them in a data.frame. It also logs some additional
#' values such as used memory and elapsed time and calls the functions 
#' `calculate_classification_metrics` and `prediction_accuracy` for more data.
#'
#' @param trained_rf the ranger object generated by training with `ranger()`
#' @param predicted_rf the ranger object generated by prediction with `predict()`.
#'   defauls to `NULL`
#' @param confusion_matrix the confusion matrix generated from the results of
#'   `predicted_rf`, defaults to `NULL`
#' @param timings the timings resulting from the `system.time()` call
#' @param n_classes the number of classes for classification
#' @param step character declaring whether `training` or `prediction`
#' @param ... parameters passed on to `prediction_accuracy`
#' 
#' @return A data frame with one row per ranger run
#'
#' @export
store_multi <- function(trained_rf, predicted_rf = NULL, 
  confusion_matrix, timings, n_classes, step, ...) {
  
  stopifnot(step == "training" | step == "prediction")
  
  results <- data.frame()
  # extract classifications for each class, every class becomes own row
  for (class in 1:n_classes) {
    results[class, "Class"] <- row.names(confusion_matrix)[class] 
    results[class, "True_positive"] <- confusion_matrix[class, class]
    results[class, "False_positive"] <- sum(confusion_matrix[,class]) - 
      confusion_matrix[class,class]
    results[class, "True_negative"] <- sum(confusion_matrix[-class, -class])
    results[class, "False_negative"] <- sum(confusion_matrix[class,]) - 
      confusion_matrix[class,class]
  }  
  # values differing between training and prediction
  if (step == "training") {
    results$Prediction_error <- trained_rf$prediction.error * 100
    results$Number_of_samples <- as.numeric(trained_rf$num.samples)   
  } else {
    results$Prediction_error <- 100 - prediction_accuracy(predicted_rf, ...)   
    results$Number_of_samples <- as.numeric(predicted_rf$num.samples)
  }
  # these values are identical or similar to extract in training and prediction
  results$Variables_sampled <- as.numeric(trained_rf$mtry)
  results$Number_independent_vars <- as.numeric(trained_rf$num.independent.variables)
  results$Tree_type <- trained_rf$treetype
  results$Seconds_elapsed <- as.numeric(timings[["elapsed"]]) 
  results$Vars_percent <- as.numeric(results$Variables_sampled / 
    results$Number_independent_vars) * 100
  results$Used_memory_Mb <- as.numeric(try(system(
    "free --mega  | grep ^Mem | tr -s ' ' | cut -d ' ' -f 3", intern = TRUE)))
  print(results)
  results <- calculate_classification_metrics(results)
  results
}

#' Calculate accuracy percentage for ranger predicted_rfs
#'
#' This function calculates predicted_rf accuracy, which means it counts the
#' amount of true positives and true negatives.
#'
#' @param predicted_rf the ranger prediction object
#' @param test_set the test_set containing the true values
#'
#' @return The accuracy in percentage
#'
#' @export
prediction_accuracy <- function(predicted_rf, test_set) {
  comparison <- cbind(predicted_rf$predictions, test_set[[ncol(test_set)]])
  matches <- ifelse(comparison[, 1]==comparison[, 2], 1, 0)
  accuracy <- sum(matches) / length(matches) * 100
  accuracy
}

#' calculate measures for classification performance
#'
#' This function calculates a bunch of classification performance metrics to help
#' evaluate the quality of the classification. For more info onto the calculated
#' values see e.g. https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers
#'
#' @param result_table the result table generated by a store_*()
#'
#' @return the result_table with additional columns
#'
#' @export
calculate_classification_metrics <- function(result_table) {
  
  if(!"Number_of_samples" %in% colnames(result_table)) {
    stop("Number_of_samples as required column is missing")
  }
    
  # general values
  result_table$Positive <- result_table$True_positive + result_table$False_negative
  result_table$Negative <- result_table$True_negative + result_table$False_positive
  result_table$Majority_label <- as.factor(ifelse(
    result_table$Negative > result_table$Positive, "Negative", "Positive"))
  result_table$Majority_fraction <- ifelse(
    result_table$Positive > result_table$Negative, 
    result_table$Positive / (result_table$Positive + result_table$Negative) * 100, 
    100 - (result_table$Positive / 
      (result_table$Positive + result_table$Negative) * 100))
  # metrics
  result_table$Accuracy <- (result_table$True_positive + result_table$True_negative)/
      result_table$Number_of_samples
  result_table$True_negative_rate <- result_table$True_negative / 
    (result_table$True_negative + result_table$False_positive)
  # make sure no NaN are created by division by 0
  result_table$True_negative_rate[is.nan(result_table$True_negative_rate)] <- 1
  result_table$Precision <- result_table$True_positive/
    (result_table$True_positive + result_table$False_positive)
  result_table$Recall <- result_table$True_positive/
    (result_table$True_positive + result_table$False_negative)
  result_table$F1_score <- 2 * ((result_table$Precision * result_table$Recall) / 
    (result_table$Precision + result_table$Recall))
  result_table$Balanced_accuracy <- (result_table$Recall + 
    result_table$True_negative_rate) / 2
  result_table
}